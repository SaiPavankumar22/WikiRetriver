{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain-community\n",
    "!pip install langchain-experimental\n",
    "!pip install langchain-groq # install the missing module\n",
    "!pip install faiss-cpu\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set API Keys\n",
    "# LANGSMITH-KEY=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\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"YOUR_LANGSMITH_KEY\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"GROQ_API_KEY\"] = \"YOUR_GROQ_API_KEY\"\n",
    "\n",
    "# Initialize components\n",
    "retriever = WikipediaRetriever()\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\")\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')  # Pre-trained embedding model\n",
    "\n",
    "# Parse HTML content and strip tags\n",
    "def parse_html(html):\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(parse_html(doc.page_content) for doc in docs)\n",
    "\n",
    "# Semantic chunking\n",
    "def semantic_chunking(text, similarity_threshold=0.75, model_name='all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Split text into sentences\n",
    "    sentences = text.split(\". \")\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_embeddings = []\n",
    "\n",
    "    for i, (sentence, embedding) in enumerate(zip(sentences, sentence_embeddings)):\n",
    "        if not current_chunk:\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_embeddings.append(embedding)\n",
    "            continue\n",
    "\n",
    "        # Compute similarity with the current chunk\n",
    "        chunk_embedding_avg = sum(current_chunk_embeddings) / len(current_chunk_embeddings)\n",
    "        similarity_score = util.cos_sim(embedding, chunk_embedding_avg).item()\n",
    "\n",
    "        if similarity_score >= similarity_threshold:\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_embeddings.append(embedding)\n",
    "        else:\n",
    "            # Finalize the current chunk\n",
    "            chunk_text = \". \".join(current_chunk).strip()\n",
    "            chunks.append({\n",
    "                \"chunk_id\": str(uuid.uuid4()),\n",
    "                \"chunk_text\": chunk_text\n",
    "            })\n",
    "            current_chunk = [sentence]\n",
    "            current_chunk_embeddings = [embedding]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunk_text = \". \".join(current_chunk).strip()\n",
    "        chunks.append({\n",
    "            \"chunk_id\": str(uuid.uuid4()),\n",
    "            \"chunk_text\": chunk_text\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Store chunks in FAISS vector database\n",
    "def store_in_faiss(chunks):\n",
    "    texts = [chunk[\"chunk_text\"] for chunk in chunks]\n",
    "    embeddings = embedder.encode(texts)\n",
    "\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return index, chunks\n",
    "\n",
    "# Re-rank outputs to return top 5 results\n",
    "def rerank_outputs(query, chunks):\n",
    "    query_embedding = embedder.encode(query)\n",
    "    chunk_embeddings = embedder.encode([chunk[\"chunk_text\"] for chunk in chunks])\n",
    "    scores = util.cos_sim(query_embedding, chunk_embeddings).squeeze().tolist()\n",
    "\n",
    "    # Combine scores with chunk IDs\n",
    "    scored_chunks = [(score, chunk[\"chunk_id\"], chunk[\"chunk_text\"]) for score, chunk in zip(scores, chunks)]\n",
    "\n",
    "    # Sort by score and return top 5 results\n",
    "    ranked = sorted(scored_chunks, key=lambda x: x[0], reverse=True)\n",
    "    return ranked[:5]  # Top 5 results\n",
    "url_retrieval_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Given the following question, find and return the Wikipedia URL(s) for the topic(s):\n",
    "    Question: {question}\n",
    "    Please provide the Wikipedia page link(s) related to this topic.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Rewrite content using LLM\n",
    "def rewrite_chunk_with_llm(chunk_text, query):\n",
    "    rewrite_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Rewrite the following content:\n",
    "\n",
    "        Content: {chunk_text}\n",
    "\n",
    "        Please provide a rewritten version of the content in a clear and more understandable.\n",
    "        donot give this type of things in the response - \"Here is a rewritten version of the content:\"\n",
    "        donot give Detailed Timeline,Related Topics,References\n",
    "        Respond with the rewritten content only.\n",
    "        \"\"\"\n",
    "    )\n",
    "    # Format the prompt\n",
    "    prompt = rewrite_prompt.format_prompt(query=query, chunk_text=chunk_text).to_string()\n",
    "\n",
    "    # Invoke the LLM with the prompt\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Extract the content directly from the response object\n",
    "    return response.content.strip()\n",
    "\n",
    "# Main execution\n",
    "# Main execution\n",
    "try:\n",
    "    query = input(\"Enter your query: \")\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    if not retrieved_docs:\n",
    "        raise ValueError(\"No documents retrieved!\")\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    retrieved_chunks = semantic_chunking(formatted_context, similarity_threshold=0.75)\n",
    "    faiss_index, faiss_chunks = store_in_faiss(retrieved_chunks)\n",
    "    ranked_results = rerank_outputs(query, faiss_chunks)\n",
    "\n",
    "\n",
    "    for rank, (score, chunk_id, chunk_text) in enumerate(ranked_results, start=1):\n",
    "        rewritten_text = rewrite_chunk_with_llm(chunk_text, query)\n",
    "        print(f\"\\n{rewritten_text}\\n<{chunk_id}>\\n\")\n",
    "\n",
    "    url_prompt = url_retrieval_prompt.format(question=query)\n",
    "    url_response = llm.invoke(url_prompt).content.strip()\n",
    "\n",
    "    print(\"\\nRelevant Wikipedia URLs:\")\n",
    "    print(url_response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during execution:\", str(e))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
